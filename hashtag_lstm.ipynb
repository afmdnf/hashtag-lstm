{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "mxlen=20\n",
    "\n",
    "char_map={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5, \"f\": 6, \"g\": 7, \"h\": 8, \"i\": 9, \"j\": 10, \"k\": 11, \"l\": 12, \"m\": 13, \"n\": 14, \"o\": 15, \"p\": 16, \"q\": 17, \"r\": 18, \"s\": 19, \"t\": 20, \"u\": 21, \"v\": 22, \"w\": 23, \"x\": 24, \"y\": 25, \"z\": 26, \"A\": 27, \"B\": 28, \"C\": 29, \"D\": 30, \"E\": 31, \"F\": 32, \"G\": 33, \"H\": 34, \"I\": 35, \"J\": 36, \"K\": 37, \"L\": 38, \"M\": 39, \"N\": 40, \"O\": 41, \"P\": 42, \"Q\": 43, \"R\": 44, \"S\": 45, \"T\": 46, \"U\": 47, \"V\": 48, \"W\": 49, \"X\": 50, \"Y\": 51, \"Z\": 52, \"0\": 53, \"1\": 54, \"2\": 55, \"3\": 56, \"4\": 57, \"5\": 58, \"6\": 59, \"7\": 60, \"8\": 61, \"9\": 62, \"_\": 63, \"UNK\":64}\n",
    "\n",
    "\n",
    "def getTrainingData(hashtags, mxlen):\n",
    "    trainingData = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(hashtags)):\n",
    "        hashtag = hashtags[i]\n",
    "        train_hashtag = []\n",
    "        label = []\n",
    "        \n",
    "        for i in range(len(hashtag)-1):\n",
    "            letter = hashtag[i]\n",
    "            next_letter = hashtag[i+1]\n",
    "            if letter != \" \":\n",
    "                if next_letter == \" \":\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "                train_hashtag.append(letter)\n",
    "                \n",
    "        if hashtag != \" \":\n",
    "            train_hashtag.append(hashtag[-1])\n",
    "            label.append(0)\n",
    "            \n",
    "        labels.append(label)\n",
    "        trainingData.append(train_hashtag[:mxlen])\n",
    "\n",
    "    return trainingData, labels\n",
    "\n",
    "# pad input sequence to fixed length\n",
    "def pad(trainingData, labels, mxlen):\n",
    "    for i in range(len(trainingData)):\n",
    "        sample = trainingData[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        if len(sample) < mxlen:\n",
    "            sample += [-1] * (mxlen-len(sample)) \n",
    "            label += [-1] * (mxlen-len(label))\n",
    "\n",
    "        sample=np.array(sample[:mxlen])\n",
    "        label=np.array(label[:mxlen])\n",
    "\n",
    "    return np.array(trainingData), labels\n",
    "\n",
    "\n",
    "def get_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        hashtag_data = f.read().split('\\n')\n",
    "        hashtag_data = [h for h in hashtag_data if len(h) > 0]\n",
    "\n",
    "    originalData, labels = getTrainingData(hashtag_data, mxlen)\n",
    "    data, labels = pad(originalData, labels, mxlen)\n",
    "\n",
    "    samples = len(data)\n",
    "    data = np.asarray(data).reshape((samples*mxlen, 1))\n",
    "    new_trainingData=[]\n",
    "\n",
    "    for char in data:\n",
    "        cc=char[0]\n",
    "        if cc == \"-1\":\n",
    "            val=0\n",
    "        else:\n",
    "            if cc in char_map:\n",
    "                val=char_map[cc]\n",
    "            else:\n",
    "                val=char_map[\"UNK\"]\n",
    "\n",
    "        new_trainingData.append(val)\n",
    "\n",
    "    labels = np.asarray(labels).reshape((samples, mxlen, 1))\n",
    "    data = np.array(new_trainingData).reshape(samples, mxlen)\n",
    "\n",
    "    return originalData, data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "originalTraining, trainingData, labels = get_data(\"train.txt\")\n",
    "originalDev, devData, devLabels = get_data(\"dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 20)            1300      \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20, 1024)          2183168   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 1)             1025      \n",
      "=================================================================\n",
      "Total params: 2,185,493\n",
      "Trainable params: 2,185,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons=512\n",
    "char_dim=20\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(char_map)+1, output_dim=char_dim, input_length=mxlen, mask_zero=True))\n",
    "\n",
    "#add Bidirectional LSTM layer here\n",
    "model.add(Bidirectional(LSTM(hidden_neurons, return_sequences=True)))\n",
    "#add Dense Time Distributed output layer here\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 705490 samples, validate on 1282 samples\n",
      "Epoch 1/2\n",
      "705490/705490 [==============================] - 4598s 7ms/step - loss: 0.1290 - acc: 0.9530 - val_loss: 0.1720 - val_acc: 0.9428\n",
      "Epoch 2/2\n",
      "705490/705490 [==============================] - 4594s 7ms/step - loss: 0.0623 - acc: 0.9774 - val_loss: 0.1376 - val_acc: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c9c3582b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=256\n",
    "NUM_EPOCHS=2\n",
    "\n",
    "# Fit model on training data, evaluating on dev data\n",
    "model.fit(trainingData, labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(devData, devLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictions to Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to segmentation\n",
    "def segment(input_seq, ys):\n",
    "    \"\"\"\n",
    "    Return the original hashtag and the segmented hashtag\n",
    "       >>> input_seq = [g, o, b, e, a, r, s, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "       >>> ys = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ,0]\n",
    "       >>> segment(input_seq, ys)\n",
    "       gobears , go bears \n",
    "    \"\"\"\n",
    "    \n",
    "    original=[]\n",
    "    segmentation=[]\n",
    "    \n",
    "    for i, char in enumerate(input_seq):\n",
    "        if char == -1:\n",
    "            break\n",
    "        original.append(char)\n",
    "        segmentation.append(char)\n",
    "        if ys[i] == 1:\n",
    "            segmentation.append(' ')\n",
    "    \n",
    "    return original, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test data written to output.txt\n",
    "out=open(\"output.txt\", \"w\")\n",
    "yhat = model.predict_classes(testData, verbose=0)\n",
    "idx=0\n",
    "samples,_,_ = yhat.shape\n",
    "for batch_num in range(samples):\n",
    "    vals=[]\n",
    "    for seq in range(mxlen):\n",
    "        vals.append(yhat[batch_num][seq][0])\n",
    "    original, segmentation=segment(originalTest[idx], vals)\n",
    "    out.write (\"%s\\t%s\\n\" % (''.join(original), ''.join(segmentation)))\n",
    "    idx+=1\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Chunking-System Evaluation F-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_F1_score(pred_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Return average F1 score of segmentations provided by the model\n",
    "    \"\"\"\n",
    "    cumF1 = count = 0\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        chunks_pred = chunks_true = 1\n",
    "        last_pred = last_true = chunks_common = 0\n",
    "        for i, label_lst in enumerate(true):\n",
    "            label = label_lst[0]\n",
    "            if label == -1:\n",
    "                if last_pred == last_true:\n",
    "                    chunks_common += 1\n",
    "                break\n",
    "\n",
    "            if label == pred[i][0]:\n",
    "                if label == 1:\n",
    "                    chunks_pred += 1\n",
    "                    chunks_true += 1\n",
    "                    if last_pred == last_true:\n",
    "                        chunks_common += 1\n",
    "                    last_true = last_pred = i\n",
    "            else:\n",
    "                if label == 1:\n",
    "                    last_true = i\n",
    "                    chunks_true += 1\n",
    "                else:\n",
    "                    last_pred = i\n",
    "                    chunks_pred += 1\n",
    "                \n",
    "        precision = 1.0*chunks_common / chunks_pred\n",
    "        recall = 1.0*chunks_common / chunks_true\n",
    "        count += 1\n",
    "        if (precision + recall) != 0:\n",
    "            cumF1 += 1.0*(2*precision*recall) / (precision + recall)\n",
    "            \n",
    "    return 1.0*cumF1 / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7538054379318021"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions for devData\n",
    "yhat = model.predict_classes(devData, verbose=0)\n",
    "segment_F1_score(yhat, devLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
